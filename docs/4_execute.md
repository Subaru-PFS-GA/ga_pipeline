# 4 Executing the GA Pipeline

Executing the pipeline consists of multiple steps:

1. First, a configuration file is created for every object, which lists all visits (exposures) that are to be processed in a single run. The `configure` script automates creating the configuration files in batch mode.

2. A batch job for each of the configurations (objects) is submitted.

3. Once all objects of a given catId are processed, a final job is executed to gather a subset of the results into a single catalog file.

## 4.1 Initialize the environment

A prerequisite to initializing the environment is a proper configuration. Please refer to Section 3 on how to configure the PFS GA Pipeline environment.

From the repository root, source the init script:

    $ cd ~/Subaru-PFS-GA/ga_pipeline
    $ source bin/init

This should activate the python environment defined in the configuration and set the `PYTHONPATH` variable for all dependencies that are available as source and not as pre-installed packages.

**TODO**: document how to initialize the environment with EUPS, once implemented

## 4.2 Configure a batch based on pfsSingle files

The `configure` script looks up objects and visits based on the pfsSingle files it can find under a given path. It then generates the pipeline config files for each object that matches the search filters. For example, executing

    $ ./bin/configure \
      --config ./configs/test/config.py \
      --datadir /datascope/subaru/data/commissioning/rerun \
      --rerundir run16/20240515 \
      --workdir /datascope/subaru/user/dobos/gapipe/temp/run16/20240709 \
      --outdir /datascope/subaru/user/dobos/gapipe/rerun/run16/20240709 \
      --catid 10015 --tract 1 --patch 1,1 \
      --objid 0x5d41-0x5d48
      
The command-line arguments are as follows.

* **--config** *config.py*: A configuration file that serves as the basis of the config files generated by the `configure` script. Config files can have the extension `.yaml`, `.json` or `.py`. In case of a Python file, the file is loaded and executed as a Python script that must create the global variable `config`.

* **--datadir** *datadir*: the root directory of PFS DRP output files.

* **--rerundir** *rerundir*: directory relative to `datadir` that contains the output files, such as `PfsSingle` of a specifc rerun.

* **--workdir** *workdir*: directory to write the config files and auxiliary output files to for each object, basically anything other than the pfsGAObject files. Log files and figures are written to the `workdir/{objId}/log` and `workdir/{objId}/fig`, respectively.

* **--outdir** *outdir*: directory to write the output data files to. Directory naming follows the DRP standard of `{outdir}/pfsGAObject/{catId}/{tract}/{patch}/`

The object ID filters, with the exception of `--patch` can be specified as a list of single values and ranges, the latter being two numerical values separated by a hyphen. Any of the object ID filters can be omitted.

* **--catid** *catid1 [catid2 ...]*: List of PFS Catalog ID filters

* **--tract** *tract1 [tract2 ...]*: List of PFS tract filters

* **--patch** *patch1*: A single value for patch. If set, the patch will be limited to that value, if omitted, objects with any patch value will be included.

* **--objid** *objid1 [objid2 ...]*: List of PFS Object ID filters. Hex notation of IDs must start with `0x`.

## 4.3 Run processing object by object

The pipeline can be executed on individual object or submitted as a batch job. To see how batch jobs are submitted, please refer to Section 4.4.

To execute the pipeline on a single object, use the `run` script and pass the path to the configuration file as an argument:

    $ ./bin/run --config /datascope/subaru/user/dobos/gapipe/rerun/run16/20240709/pfsGAObject/10015/00001/1,1/pfsGAObject-10015-00001-1,1-0000000000005d41-007-0x34957c6298e62be0.yaml

The `run` script will execute the pipeline on the object specified in the configuration file. The output will be written to the `outdir` directory specified in the configuration file.

It is possible to override the input and output directories defined in the configuration file by passing additional command-line arguments. The following arguments are available:

* **--datadir** *datadir*

* **--rerundir** *rerundir*

* **--workdir** *workdir*

* **--outdir** *outdir*

See Section 4.2 for a description of these arguments.

# 4.4 Submitting a batch job

# 4.5 Collecting the results

# 4.6 Additional command-line arguments

* **--debug**: Enable debug mode. The scripts are executed within a `debugpy` context.

* **--profile**: Enable profiling. The scripts are executed within a `cProfile` context.

* **--log-level** *level*: Set the log level. The default is `INFO` but can be set to `TRACE`, `DEBUG`, `WARNING`, `ERROR`, or `CRITICAL`.