# 3 Running the GA Pipeline

Executing the pipeline consists of multiple steps:

* First, the individual spectra are extracted from the container files, such as PfsSingle files from PfsCalibrated files. This step is optional but can speed up the processing and reduce memory use. Running `gapipe-repo extract` extracts the individual spectra into the `$GAPIPE_WORKDIR` staging directory.

* Second, a configuration file is created for every object, which lists all visits (exposures) that are to be processed in a single GAPIPE run. The `./bin/configure` script automates creating the configuration files in batch mode.

* A batch job for each of the configurations (objects) is submitted. Alternatively, individual and batch jobs can be manually executed from the command-line. The command-line script `./bin/run` executes the pipeline on a single object, using the configuration file as input.

3. Once all objects of a given run are processed, a final command is executed to collect (a subset) of the results into a single catalog file.

## 3.1 Initializing the environment

Please note that the following is preliminary and assumes that GAPIPE is installed from "source" and not as a pre-packaged Python package, either Acanconda or EUPS.

In development mode, a prerequisite to initializing the environment is a proper configuration. Please refer to Section 1 on how to configure the GAPIPE environment. When properly configured, the environment can be initialized from the root directory, source the init script:

    $ cd $GAPIPE_DIR/src/ga_pipeline
    $ source bin/init

where `$GAPIPE_DIR` is the root directory of the GAPIPE installation.

This should activate the python environment defined in the default environment file and set the `PYTHONPATH` variable for all dependencies that are available as source and not as pre-installed packages.

Once the environment is initialized, a set of commands starting with `gapipe-` are available in the command-line.

For the sake of simplicity, we define a few environmental varibles that are used in the examples of the following sections. Some of these read the observation logs available in the `spt_ssp_observation` repository. The IDs refer to valid data in the March, 2025 observing run.

    $ OBSRUN="2025-03"
    $ RERUN="run21_June2025"
    $ BUTLER_COLLECTIONS="$RERUN"
    $ CATID="10092"
    $ OBJID="0x600000d9d"
    $ OBSLOGS="spt_ssp_observation/runs/$OBSRUN/obslog/*.csv"
    $ VISITS="$(cat $OBSLOGS | grep SSP_GA | cut -d ',' -f 1)"

## 3.2 Configure GAPIPE to process a batch of objects

Before executing the pipeline, a configuration file must be created for each object that is to be processed. The configuration file contains the list of visits (exposures) that are to be processed in a single GAPIPE run and all necessary settings for the pipeline spectrum processing steps. Since many of the configuration options are common for all objects, the configuration file is generated from a template configuration file. Some templates are available in the `configs` directory of the GAPIPE source code.

To run the configuration script, all data files must be available, even when using Butler to access the data, because the `PfsConfig` files are necessary to generate the configuration files. In addition, files containing the photometric stellar parameters (to define the priors), as well as observational parameters, 

The `gapipe-configure` script  generates the pipeline config files for each object that matches the search filters. For example:

    $ gapipe-configure --config ./configs/gapipe/$RERUN/single.py --objid $OBJID --visit $VISITS --obs-logs $OBSLOGS

* The `--config` parameter sets the file that serves as the template of the config files generated by the `configure` script. Config files can have the extension `.yaml`, `.json` or `.py`. In case of a Python file, the file is loaded and executed as a Python script that must create the global variable `config`.

To generate the configuration files for all objects that match the search filters, you should remove the `--objid` argument. For example:

    $ gapipe-configure --config ./configs/gapipe/$RERUN/single.py --visit $VISITS --obs-logs $OBSLOGS --catid $CATID --targettype SCIENCE

Specify the `--progress` option to see a progress bar and the number of objects processed so far. You can use the `--top` option to limit the number of objects to process. This is useful for testing the configuration script and the pipeline processing steps.

See the reference section for a complete list of command-line arguments.

## 3.2.1 Specifying the stellar parameters to define the priors

In order to generate config files with parameter priors that depend on the photometric stellar parameters, known velocities, etc, use the `--stellar-params` and `--obs-params` options to specify the files containing the stellar parameters and observational parameters, respectively. For example:

    $ gapipe-configure --config ./configs/gapipe/$RERUN/single.py --objid $OBJID --visit $VISITS --obs-logs $OBSLOGS --stellar-params stellar_params.csv --obs-params obs_params.csv

TBW: write about the format of the stellar parameters and observational parameters files.

## 3.3 Run processing object by object

The pipeline can be executed on individual object or submitted as a batch job. To see how batch jobs are submitted, please refer to Section 4.4.

To execute the pipeline on a single object, use the `run` script and pass the path to the configuration file as an argument:

    $ gapipe-run --config $GAPIPE_WORKDIR/$RERUN/pfsStar/10092/0000000600000d9d-031-0x7270e4b76e8cc231/pfsStar-10092-0000000600000d9d-031-0x7270e4b76e8cc231.yaml

The `run` script will execute the pipeline on the object specified in the configuration file. The output will be written to the `outdir` directory specified in the configuration file.

It is possible to override the input and output directories defined in the configuration file by passing additional command-line arguments. The following arguments are available:

* **--datadir** *datadir*

* **--rerundir** *rerundir*

* **--workdir** *workdir*

* **--outdir** *outdir*

See Section 4.2 for a description of these arguments.

# 3.4 Submitting a batch job

    $ gapipe-run --rerundir $RERUN --catid $CATID --batch slurm --partition v100 --cpus 2 --mem 12G --top 100 --progress

# 3.5 Cataloging the results

    $ gapipe-catalog --obs-log $OBSLOGS --objid 0x600000000-0x6FFFFFFFF

# 3.6 Additional command-line arguments

* **--debug**: Enable debug mode. The scripts are executed within a `debugpy` context.

* **--profile**: Enable profiling. The scripts are executed within a `cProfile` context.

* **--log-level** *level*: Set the log level. The default is `INFO` but can be set to `TRACE`, `DEBUG`, `WARNING`, `ERROR`, or `CRITICAL`.